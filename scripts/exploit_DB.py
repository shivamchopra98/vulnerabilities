import os
import requests
import hashlib
import shutil
import pandas as pd
from datetime import datetime

# -------------------------
# Configuration (edit if needed)
# -------------------------
PROJECT_ROOT = r"C:\Users\ShivamChopra\Projects\vulnerabilities"
EXPLOIT_SUBDIR = "exploit_database"
BASE_DB_DIRNAME = "base_database"
BASE_DB_FILENAME = "base_DB.csv"

# Raw CSV URL (use the raw file, not the blob HTML page)
RAW_CSV_URL = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"

# -------------------------
# Derived paths
# -------------------------
BASE_PATH = os.path.join(PROJECT_ROOT, EXPLOIT_SUBDIR)                      # folder for dated folders + base_db folder
BASE_DB_DIR = os.path.join(BASE_PATH, BASE_DB_DIRNAME)                     # folder that contains base_DB.csv
BASE_DB_PATH = os.path.join(BASE_DB_DIR, BASE_DB_FILENAME)                 # full path to base DB file

TODAY = datetime.now().strftime("%Y-%m-%d")
TODAY_FOLDER = os.path.join(BASE_PATH, TODAY)                              # folder to save today's download
TODAY_FILE = os.path.join(TODAY_FOLDER, f"files_exploits_{TODAY}.csv")      # path for today's CSV

# -------------------------
# Utility helpers
# -------------------------
def ensure_dirs():
    os.makedirs(BASE_PATH, exist_ok=True)
    os.makedirs(BASE_DB_DIR, exist_ok=True)
    os.makedirs(TODAY_FOLDER, exist_ok=True)

def download_raw_csv(url, save_path):
    print(f"‚¨áÔ∏è Downloading CSV from {url} ‚Üí {save_path}")
    resp = requests.get(url, stream=True, timeout=60)
    resp.raise_for_status()
    with open(save_path, "wb") as f:
        for chunk in resp.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
    print("‚úÖ Download complete")

def cleanup_old_dated_folders(keep_folder):
    """Remove dated folders inside BASE_PATH except keep_folder and the base_db folder."""
    for entry in os.listdir(BASE_PATH):
        p = os.path.join(BASE_PATH, entry)
        if not os.path.isdir(p):
            continue
        # keep the base_db folder and today's folder
        if os.path.basename(p) == BASE_DB_DIRNAME:
            continue
        if os.path.abspath(p) == os.path.abspath(keep_folder):
            continue
        try:
            shutil.rmtree(p)
            print(f"üóëÔ∏è Removed old dated folder {p}")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to remove {p}: {e}")

def guess_id_column(df: pd.DataFrame):
    # Common columns in exploit CSVs: 'id', 'EDB-ID', 'exploit_id', 'id' etc.
    lowered = {c.lower(): c for c in df.columns}
    candidates = ["id", "edb_id", "edb-id", "exploit_id", "exploit-id", "exploitdb_id", "cve", "cve_id"]
    for cand in candidates:
        if cand in lowered:
            return lowered[cand]
    # fallback: find any numeric unique column
    for col in df.columns:
        try:
            if pd.api.types.is_integer_dtype(df[col]) or pd.api.types.is_numeric_dtype(df[col]):
                if df[col].is_unique:
                    return col
        except Exception:
            pass
    return None

def row_hash_series(df: pd.DataFrame):
    def row_hash(row):
        s = "|".join([str(x) for x in row.values])
        return hashlib.sha1(s.encode("utf-8", errors="replace")).hexdigest()
    return df.apply(row_hash, axis=1)

def merge_and_update_base(base_path, new_path):
    # read CSVs with dtype=str to preserve formatting, avoid NaNs
    new_df = pd.read_csv(new_path, dtype=str, keep_default_na=False)
    new_df.columns = [c.strip() for c in new_df.columns]

    if not os.path.exists(base_path):
        print("‚ö†Ô∏è Base DB not found ‚Äî creating base DB from downloaded CSV.")
        new_df.to_csv(base_path, index=False, encoding="utf-8")
        return {"created": True, "added": len(new_df)}

    base_df = pd.read_csv(base_path, dtype=str, keep_default_na=False)
    base_df.columns = [c.strip() for c in base_df.columns]

    # Align columns: ensure both dataframes have the same set of columns (add missing as empty)
    for c in base_df.columns:
        if c not in new_df.columns:
            new_df[c] = ""
    for c in new_df.columns:
        if c not in base_df.columns:
            base_df[c] = ""

    # Reorder new_df to base_df columns for consistent comparison
    new_df = new_df[base_df.columns]

    id_col = guess_id_column(base_df) or guess_id_column(new_df)
    if id_col:
        print(f"‚ÑπÔ∏è Using identifier column for deduplication: '{id_col}'")
        base_ids = set(base_df[id_col].astype(str).tolist())
        mask_new = ~new_df[id_col].astype(str).isin(base_ids)
        additions = new_df[mask_new]
        added_count = len(additions)
        if added_count > 0:
            merged = pd.concat([base_df, additions], ignore_index=True, sort=False)
            merged.to_csv(base_path, index=False, encoding="utf-8")
            print(f"‚ûï Appended {added_count} new rows to base DB (by id).")
        else:
            print("‚ÑπÔ∏è No new rows found (by id).")
        return {"method": "id", "id_col": id_col, "added": added_count}
    else:
        print("‚ÑπÔ∏è No reliable id column detected ‚Äî falling back to row-hash comparison.")
        base_hashes = set(row_hash_series(base_df).tolist())
        new_hashes = row_hash_series(new_df)
        mask_new = ~new_hashes.isin(base_hashes)
        additions = new_df[mask_new.values]
        added_count = len(additions)
        if added_count > 0:
            merged = pd.concat([base_df, additions], ignore_index=True, sort=False)
            merged.to_csv(base_path, index=False, encoding="utf-8")
            print(f"‚ûï Appended {added_count} new rows to base DB (by row-hash).")
        else:
            print("‚ÑπÔ∏è No new rows found (by row-hash).")
        return {"method": "row-hash", "added": added_count}

# -------------------------
# Main flow
# -------------------------
def main():
    ensure_dirs()
    try:
        download_raw_csv(RAW_CSV_URL, TODAY_FILE)
    except Exception as e:
        print(f"‚ùå Download failed: {e}")
        return

    # Merge/update base DB
    try:
        result = merge_and_update_base(BASE_DB_PATH, TODAY_FILE)
        print("Result:", result)
    except Exception as e:
        print("‚ùå Merge/update error:", e)
        return

    # cleanup today's file/folder (delete folder after verifying file exists)
    try:
        if os.path.exists(TODAY_FOLDER):
            shutil.rmtree(TODAY_FOLDER)
            print(f"üóëÔ∏è Removed temporary folder {TODAY_FOLDER}")
    except Exception as e:
        print("‚ö†Ô∏è Could not remove temp folder:", e)

    # Optionally: remove other dated folders (keep only today's was already removed so nothing remains)
    cleanup_old_dated_folders(TODAY_FOLDER)

if __name__ == "__main__":
    main()
